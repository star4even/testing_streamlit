# -*- coding: utf-8 -*-
"""Capstone_Rev_90_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10EAgmWevnOPpZ_oPVGuPPxHbCeV5AwxD
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

# 1. Load and prepare the data
def load_data(file_path):
    """Load the time series production data from a CSV file"""
    try:
        df = pd.read_csv("2021-2024.csv")
        print("Available columns:", df.columns.tolist())

        # Convert the Date column using dayfirst=True for DD/MM/YYYY format
        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
        return df
    except Exception as e:
        print(f"Error loading data: {e}")
        return None

# 2. Handle missing values
def handle_missing_values(df):
    """More robust missing value handling"""
    # Forward fill first
    df_filled = df.ffill()
    # Then backward fill
    df_filled = df_filled.bfill()
    # Optionally add interpolation
    df_filled = df_filled.interpolate(method='time')
    return df_filled

from scipy import stats

def remove_outliers(df, z_threshold=3):
    """Remove outliers using z-score"""
    z_scores = stats.zscore(df)
    return df[(np.abs(z_scores) < z_threshold).all(axis=1)]

# 3. Feature engineering for time series data
def create_features(df, target_col):
    """
    Create time series features based on date index and rolling statistics
    Returns:
        - df_features: DataFrame with all features
        - feature_columns: List of feature names used for modeling
    """
    df_features = df.copy()

    # Extract date features
    df_features['day_of_week'] = df_features.index.dayofweek
    df_features['month'] = df_features.index.month
    df_features['quarter'] = df_features.index.quarter
    df_features['year'] = df_features.index.year
    df_features['day_of_year'] = df_features.index.dayofyear

    # Calculate moving averages for each production metric
    for col in df.columns:
        df_features[f'{col}_MA3'] = df_features[col].rolling(window=3).mean()
        df_features[f'{col}_MA7'] = df_features[col].rolling(window=7).mean()
        df_features[f'{col}_MA14'] = df_features[col].rolling(window=14).mean()

    # Create lag features for all columns
    for col in df.columns:
        for lag in range(1, 8):
            df_features[f'{col}_lag_{lag}'] = df_features[col].shift(lag)

    df_features.dropna(inplace=True)

    # Get the list of feature columns (exclude the target)
    feature_columns = [col for col in df_features.columns if col != target_col]

    return df_features, feature_columns

def create_features(df, target_col):
    """Enhanced feature engineering"""
    df_features = df.copy()

    # Existing date features
    df_features['day_of_week'] = df_features.index.dayofweek
    df_features['month'] = df_features.index.month
    df_features['quarter'] = df_features.index.quarter
    df_features['year'] = df_features.index.year
    df_features['day_of_year'] = df_features.index.dayofyear

    # Existing moving averages and lags
    for col in df.columns:
        df_features[f'{col}_MA3'] = df_features[col].rolling(window=3).mean()
        df_features[f'{col}_MA7'] = df_features[col].rolling(window=7).mean()
        df_features[f'{col}_MA14'] = df_features[col].rolling(window=14).mean()

        for lag in range(1, 8):
            df_features[f'{col}_lag_{lag}'] = df_features[col].shift(lag)

    # New features
    df_features['rolling_std_7'] = df_features[target_col].rolling(window=7).std()
    df_features['rolling_max_14'] = df_features[target_col].rolling(window=14).max()
    df_features['rolling_min_14'] = df_features[target_col].rolling(window=14).min()
    df_features['monthly_avg'] = df_features.groupby('month')[target_col].transform('mean')

    # Add Fourier terms for seasonality
    for n in range(1, 4):
        df_features[f'fourier_sin_{n}'] = np.sin(2 * np.pi * n * df_features['day_of_year']/365)
        df_features[f'fourier_cos_{n}'] = np.cos(2 * np.pi * n * df_features['day_of_year']/365)

    df_features.dropna(inplace=True)

    return df_features, [col for col in df_features.columns if col != target_col]

# 4. Prepare data for XGBoost modeling
def prepare_for_modeling(df_features, feature_columns, target_col, forecast_horizon=90):
    """
    Prepare the dataset for XGBoost modeling with a specific forecast horizon
    """
    df_model = df_features.copy()
    df_model['target'] = df_model[target_col].shift(-forecast_horizon)
    df_model.dropna(subset=['target'], inplace=True)

    X = df_model[feature_columns]
    y = df_model['target']

    return X, y

# 5. Scale the features
def scale_features(X_train, X_test):
    """Scale the features using StandardScaler"""
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled, scaler

from sklearn.model_selection import RandomizedSearchCV

def tune_xgboost(X_train, y_train):
    """Hyperparameter tuning for XGBoost"""
    param_dist = {
        'n_estimators': [100, 500, 1000],
        'learning_rate': [0.001, 0.01, 0.1],
        'max_depth': [3, 5, 7],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'gamma': [0, 0.1, 0.2]
    }

    xgb = xgb.XGBRegressor(random_state=42)
    random_search = RandomizedSearchCV(
        estimator=xgb,
        param_distributions=param_dist,
        n_iter=20,
        cv=3,
        verbose=2,
        random_state=42,
        n_jobs=-1
    )

    random_search.fit(X_train, y_train)
    return random_search.best_estimator_

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def evaluate_model(model, X_test, y_test):
    """Comprehensive model evaluation"""
    predictions = model.predict(X_test)

    metrics = {
        'MAE': mean_absolute_error(y_test, predictions),
        'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
        'R2': r2_score(y_test, predictions)
    }

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': X_test.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    return metrics, feature_importance

from sklearn.model_selection import TimeSeriesSplit

def time_series_cv(model, X, y, n_splits=5):
    """Time series cross-validation"""
    tscv = TimeSeriesSplit(n_splits=n_splits)
    scores = []

    for train_index, test_index in tscv.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, preds))
        scores.append(rmse)

    return np.mean(scores), np.std(scores)

# 6. Forecast future values - CORRECTED VERSION
def forecast_future(model, last_data, scaler, feature_columns, target_col, forecast_horizon=90):
    """
    Forecast future values recursively
    """
    forecast_dates = pd.date_range(start=last_data.index[-1] + pd.Timedelta(days=1), periods=forecast_horizon)
    forecast_values = []

    current_data = last_data.copy()

    for _ in range(forecast_horizon):
        # Prepare the next date
        next_date = current_data.index[-1] + pd.Timedelta(days=1)

        # Create a new row with all required features
        new_row = pd.DataFrame(index=[next_date])

        # 1. Add date features
        new_row['day_of_week'] = next_date.dayofweek
        new_row['month'] = next_date.month
        new_row['quarter'] = next_date.quarter
        new_row['year'] = next_date.year
        new_row['day_of_year'] = next_date.dayofyear

        # 2. Add the latest values for all original columns
        for col in last_data.columns:
            if col in feature_columns and col not in new_row.columns:
                new_row[col] = current_data[col].iloc[-1]

        # 3. Update lag features (using previous values)
        for col in last_data.columns:
            if col != 'target':
                for lag in range(1, 8):
                    lag_col = f'{col}_lag_{lag}'
                    if lag_col in feature_columns:
                        if lag <= len(current_data):
                            new_row[lag_col] = current_data[col].iloc[-lag]
                        else:
                            new_row[lag_col] = np.nan

        # 4. Update moving averages
        for col in last_data.columns:
            if col != 'target':
                for ma_window in [3, 7, 14]:
                    ma_col = f'{col}_MA{ma_window}'
                    if ma_col in feature_columns:
                        new_row[ma_col] = current_data[col].rolling(ma_window).mean().iloc[-1]

        # Ensure we only keep the features used in training
        new_row = new_row[feature_columns]

        # Fill any remaining NaNs (for initial lags/MAs)
        new_row = new_row.ffill().bfill()

        # Scale features
        new_row_scaled = scaler.transform(new_row)

        # Make prediction - CHANGED THIS PART
        # Convert to numpy array and ensure correct shape
        pred = model.predict(new_row_scaled.reshape(1, -1))[0]
        forecast_values.append(pred)

        # Update current_data with this prediction
        current_data.loc[next_date, target_col] = pred

        # Update all derived columns for the new row
        for col in last_data.columns:
            if col != target_col and col in feature_columns:
                current_data.loc[next_date, col] = new_row[col].values[0]

    forecast_df = pd.DataFrame({
        'Date': forecast_dates,
        'Forecasted Value': forecast_values
    }).set_index('Date')

    return forecast_df

def forecast_future_enhanced(model, last_data, scaler, feature_columns, target_col, forecast_horizon=90, ci_level=0.95):
    """Enhanced forecasting with confidence intervals"""
    forecast_dates = pd.date_range(start=last_data.index[-1] + pd.Timedelta(days=1), periods=forecast_horizon)
    forecast_values = []
    lower_bounds = []
    upper_bounds = []

    current_data = last_data.copy()

    for _ in range(forecast_horizon):
        next_date = current_data.index[-1] + pd.Timedelta(days=1)
        new_row = pd.DataFrame(index=[next_date])

        # Add all features as before
        new_row['day_of_week'] = next_date.dayofweek
        new_row['month'] = next_date.month
        new_row['quarter'] = next_date.quarter
        new_row['year'] = next_date.year
        new_row['day_of_year'] = next_date.dayofyear

        for col in last_data.columns:
            if col in feature_columns and col not in new_row.columns:
                new_row[col] = current_data[col].iloc[-1]

        # Update lag features
        for col in last_data.columns:
            if col != 'target':
                for lag in range(1, 8):
                    lag_col = f'{col}_lag_{lag}'
                    if lag_col in feature_columns:
                        if lag <= len(current_data):
                            new_row[lag_col] = current_data[col].iloc[-lag]
                        else:
                            new_row[lag_col] = np.nan

        # Update moving averages
        for col in last_data.columns:
            if col != 'target':
                for ma_window in [3, 7, 14]:
                    ma_col = f'{col}_MA{ma_window}'
                    if ma_col in feature_columns:
                        new_row[ma_col] = current_data[col].rolling(ma_window).mean().iloc[-1]

        new_row = new_row[feature_columns]
        new_row = new_row.ffill().bfill()
        new_row_scaled = scaler.transform(new_row)

        # Get prediction and confidence intervals
        pred = model.predict(new_row_scaled.reshape(1, -1))[0]

        # For confidence intervals (simplified approach)
        # In production, consider using quantile regression or Bayesian methods
        residuals = y_train - model.predict(X_train_scaled)
        std_dev = np.std(residuals)
        z_score = stats.norm.ppf(1 - (1 - ci_level)/2)
        margin = z_score * std_dev

        forecast_values.append(pred)
        lower_bounds.append(pred - margin)
        upper_bounds.append(pred + margin)

        # Update current_data with this prediction
        current_data.loc[next_date, target_col] = pred
        for col in last_data.columns:
            if col != target_col and col in feature_columns:
                current_data.loc[next_date, col] = new_row[col].values[0]

    forecast_df = pd.DataFrame({
        'Date': forecast_dates,
        'Forecasted_Value': forecast_values,
        'Lower_Bound': lower_bounds,
        'Upper_Bound': upper_bounds
    }).set_index('Date')

    return forecast_df

# 7. Train XGBoost Model - CORRECTED VERSION
def train_xgboost(X_train, y_train):
    """Train the XGBoost model with the provided training data"""
    model = xgb.XGBRegressor(
        n_estimators=1000,
        learning_rate=0.01,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        early_stopping_rounds=50,
        random_state=42
    )

    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=True
    )

    return model

def plot_forecast(df_features, forecast, target_col):
    """Enhanced forecast visualization"""
    plt.figure(figsize=(14, 7))

    # Historical data
    plt.plot(df_features.index, df_features[target_col], label='Historical Data', color='blue')

    # Forecast
    plt.plot(forecast.index, forecast['Forecasted_Value'], label='Forecast', color='red')

    # Confidence interval
    plt.fill_between(
        forecast.index,
        forecast['Lower_Bound'],
        forecast['Upper_Bound'],
        color='pink', alpha=0.3, label='95% Confidence Interval'
    )

    plt.title(f'{target_col} - Historical Data and Forecast')
    plt.xlabel('Date')
    plt.ylabel(target_col)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    # Add vertical line at forecast start
    forecast_start = forecast.index[0]
    plt.axvline(x=forecast_start, color='green', linestyle='--', alpha=0.7)

    plt.show()

def main_enhanced():
    """Enhanced main function with all improvements"""
    print("Loading data...")
    file_path = "2021-2024.csv"
    df = load_data(file_path)

    if df is None:
        print("Failed to load data. Exiting.")
        return None, None

    print("\nHandling missing values and outliers...")
    df_clean = handle_missing_values(df)
    df_clean = remove_outliers(df_clean)

    target_col = 'Production KPC (kt)'
    if target_col not in df_clean.columns:
        print(f"Target column '{target_col}' not found in data.")
        return None, None

    print("\nCreating enhanced time series features...")
    df_features, feature_columns = create_features(df_clean, target_col)

    print("\nPreparing data for modeling...")
    X, y = prepare_for_modeling(df_features, feature_columns, target_col)

    print("\nSplitting data...")
    train_size = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]

    print("\nScaling features...")
    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)

    print("\nHyperparameter tuning...")
    model = tune_xgboost(X_train_scaled, y_train)

    print("\nTraining final model...")
    model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_test_scaled, y_test)],
        verbose=True
    )

    print("\nEvaluating model...")
    metrics, feature_importance = evaluate_model(model, X_test_scaled, y_test)
    print("\nModel Metrics:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")

    print("\nFeature Importance:")
    print(feature_importance.head(10))

    print("\nTime Series Cross-Validation...")
    cv_mean, cv_std = time_series_cv(model, X, y)
    print(f"CV RMSE: {cv_mean:.4f} Â± {cv_std:.4f}")

    print("\nForecasting next 90 days...")
    last_data = df_features.iloc[-90:]  # Use last 90 days for forecasting
    forecast = forecast_future_enhanced(model, last_data, scaler, feature_columns, target_col)

    print("\nForecast for the next 90 days:")
    print(forecast.head())

    plot_forecast(df_features, forecast, target_col)

    return model, forecast, metrics

def main():
    try:
        # 1. Data Loading
        print("1. Loading data...")
        df = load_data("2021-2024.csv")
        print("Data shape:", df.shape)
        display(df.head())

        # 2. Data Cleaning
        print("\n2. Cleaning data...")
        df_clean = handle_missing_values(df)
        print("Missing values after cleaning:", df_clean.isnull().sum().sum())

        # 3. Feature Engineering
        print("\n3. Creating features...")
        df_features, feature_columns = create_features(df_clean, 'Production KPC (kt)')
        print(f"Created {len(feature_columns)} features")
        display(df_features.tail())

        # 4. Model Preparation
        print("\n4. Preparing for modeling...")
        X, y = prepare_for_modeling(df_features, feature_columns, 'Production KPC (kt)')
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        print(f"Train size: {len(X_train)}, Test size: {len(X_test)}")

        # 5. Model Training
        print("\n5. Training model...")
        model = train_xgboost(X_train, y_train)
        print("Training complete")

        # 6. Forecasting
        print("\n6. Forecasting...")
        last_data = df_features.iloc[-90:]
        forecast = forecast_future(model, last_data, scaler, feature_columns, 'Production KPC (kt)')
        print("\nForecast results:")
        display(forecast.head())

        # 7. Plotting
        print("\n7. Generating plot...")
        plt.figure(figsize=(14, 7))
        plt.plot(df_features.index, df_features['Production KPC (kt)'], label='Historical')
        plt.plot(forecast.index, forecast['Forecasted Value'], label='Forecast', color='red')
        plt.title('Production Forecast')
        plt.legend()
        plt.show()

        return model, forecast

    except Exception as e:
        print(f"Error in main(): {str(e)}")
        return None, None

# Execute and display results
if __name__ == "__main__":
    model, forecast = main()

    if forecast is not None:
        print("\nFinal forecast stats:")
        print(forecast.describe())

def main():
    try:
        print("Loading data...")
        file_path = "2021-2024.csv"
        df = load_data(file_path)

        if df is None:
            print("Failed to load data. Exiting.")
            return None, None

        print("\nHandling missing values...")
        df_clean = handle_missing_values(df)

        target_col = 'Production KPC (kt)'
        if target_col not in df_clean.columns:
            print(f"Target column '{target_col}' not found.")
            return None, None

        print("\nCreating time series features...")
        df_features, feature_columns = create_features(df_clean, target_col)

        print("\nPreparing data for modeling...")
        X, y = prepare_for_modeling(df_features, feature_columns, target_col)

        print("\nSplitting data...")
        train_size = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
        y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]

        # THIS IS THE CRUCIAL FIX - CREATE AND STORE SCALER
        print("\nScaling features...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        print("\nTraining XGBoost model...")
        model = train_xgboost(X_train_scaled, y_train)

        print("\nForecasting next 90 days...")
        last_data = df_features.iloc[-90:]  # Use last 90 days for forecasting

        # NOW SCALER IS AVAILABLE HERE
        forecast = forecast_future(model, last_data, scaler, feature_columns, target_col)

        print("\nForecast for the next 90 days:")
        print(forecast.head())

        # Plot results
        plt.figure(figsize=(14, 7))
        plt.plot(df_features.index, df_features[target_col], label='Historical Data')
        plt.plot(forecast.index, forecast['Forecasted Value'], label='Forecast', color='red')
        plt.title(f'{target_col} - Historical Data and Forecast')
        plt.xlabel('Date')
        plt.ylabel(target_col)
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

        return model, forecast

    except Exception as e:
        print(f"Error in main(): {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None


# Execute the main function
if __name__ == "__main__":
    model, forecast = main()

    if forecast is not None:
        print("\nForecast summary:")
        print(forecast.describe())